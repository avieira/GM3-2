\section{Présentation des méthodes d'Euler et d'Adams-Bashforth-2}
\subsection{Méthode d'Euler}
Cette méthode est utilisée pour initialiser la méthode d'Adams. Elle a en effet besoin de deux points à chaque itération, or, le problème de Cauchy n'en donne qu'un seul.
\subsubsection{Définition de la méthode}
Pour définir la méthode d'Euler, repartons du système différentiel, qu'on définit sur $[x_t,x_{t+1}]$, $t\in\mathbb{N}$, intervalle de longueur h :
\[\left\{ \begin{array}{c c c}
	dy = f(x,y) dx \\
	y(x_t)=y_t
\end{array}\right.\]
Ce système équivaut à :
\[\int_{y_t}^{y_{t+1}} dy = \int_{x_t}^{x_{t+1}} f(x,y) dx\]
On approche la deuxième intégrale par la méthode des rectangles à gauche :
\[\int_{x_t}^{x_{t+1}} f(x,y) dx \approx \underbrace{(x_{t+1}-x_t)}_{=h}f(x_t,y_t)\]

Ainsi :
\[y_{t+1} \approx y_t + hf(x_t,y_t)\]

\subsubsection{Stabilité et convergence}
\begin{itemize}
	\item Si $f$ est lipschitzienne par rapport à y, la méthode est stable.
	\item La méthode est convergente.
\end{itemize}

\subsection{Méthode d'Adams-Bashforth-2}
\subsubsection{Définition de la méthode}

L'idée de cette méthode est simple : on interpole à l'ordre 1 la fonction $f(x,y)$ entre $x_{t-1}$ et $x_t$. Ainsi :
\begin{eqnarray*}
f(x,y) &\approx& f(x_t,y_t)+\frac{f(x_{t-1},y_{t-1})-f(x_t,y_t)}{x_{t-1}-x_t}(x-x_t)\\
       &\approx& f_t + \frac{f_{t-1}-f_t}{x_{t-1}-x_t}(x-x_t)
\end{eqnarray*}
On pose $L_2(f,x)=f_t + \frac{f_{t-1}-f_t}{x_{t-1}-x_t}(x-x_t)$.

\bigskip
On repart à présent du système différentiel de départ, qu'on définit entre $x_t$ et $x_{t+1}$. On considère qu'entre chaque $x_i$ et $x_{i+1}$, on a un pas constant $h$ :
\[ \left\{ \begin{array}{c c c}
	\frac{dy}{dx} &=& f(x,y) \\
		y(x_t)&=&y_t
\end{array}\right.\]

Ce système équivaut à : 
\[y_{t+1}=y_t + \int_{x_t}^{x_{t+1}} f(x,y) dx\]
On utilise ici l'approximation de $f$. On calcule pour cela $L_2(f,x_t)$ et $L_2(f,x_{t+1})$.
\begin{eqnarray*}
	L_2(f,x_t)&=&f_t\\
	L_2(f,x_{t+1})&=&f_t + \frac{f_{t-1}-f_t}{\underbrace{x_{t-1}-x_t}_{=-h}}\underbrace{(x_{t+1}-x_t)}_{=h}\\
			&=&2f_t - f_{t-1} 
\end{eqnarray*}

D'où :
\begin{eqnarray*}
	\int_{x_t}^{x_{t+1}} f(x,y)dx &\approx& \int_{x_t}^{x_{t+1}} L_2(f,x) dx \\
				      &\approx& \frac{h}{2}(L_2(f,x_{t+1}) + L_2(f,x_t))\\
			       	      &\approx& \frac{h}{2}(3f_t - f_{t-1})
\end{eqnarray*}

D'où la méthode d'Adams-Bashforth-2 :
\[y_{t+1}=y_t + \frac{h}{2}(3f_t - f_{t-1})\]

\subsubsection{Convergence de la méthode}
Démontrons qu'il existe $\eta$ tel que : 
\[\varepsilon(t,h)=\frac{5}{12} h^3 y^{(3)}(\eta)\]
Où $\varepsilon(t,h)$ est l'erreur sur la méthode.\\
Considérons d'abord l'approximation de la solution au point t :

\begin{eqnarray*}
	y(t) &\approx& y(t-h) + \frac{h}{2} [3f(t-h,y(t-h)) - f(t-2h,y(t-2h))] \\
	     &\approx& y(t-h) + \frac{h}{2} [3y'(t-h) - y'(t-2h)]
\end{eqnarray*}

On utilise à présent la formule de Taylor :
\begin{eqnarray*}
	y(t-h)&=&y(t)-hy'(t)+\frac{h^2}{2}y''(t)-\frac{h^3}{6}y^{(3)}(\eta)\\
       y'(t-h)&=&y'(t)-hy''(t)+\frac{h^2}{2}y^{(3)}(\eta)\\
      y'(t-2h)&=&y'(t)-2hy''(t)+2h^2y^{(3)}(\eta)
\end{eqnarray*}

On a donc :
\begin{eqnarray*}
	\varepsilon(t,h)&=&y(t)-y(t-h)-\frac{h}{2}[3y'(t-h)-y'(t-2h)]\\
			&=&hy'(t)-\frac{h^2}{2}y''(t)+\frac{h^3}{6}y^{(3)}(\eta)
	-\frac{3h}{2}y'(t) + \frac{3h^2}{2}y''(t) - \frac{3h^3}{4}y^{(3)}(t)
	+\frac{h}{2}y'(t)-h^2y''(t)+h^3y^{(3)}(\eta)\\
			&=&hy'(t)\left( 1-\frac{3}{2}+\frac{1}{2}\right) + h^2y''(t)\left( -\frac{1}{2} + \frac{3}{2}-1\right) 
	+ h^3y^{(3)}(\eta)\left(\frac{1}{6}-\frac{3}{4}+1\right) \\
			&=&\frac{5}{12}h^3y^{(3)}(\eta)
\end{eqnarray*}
Donc la méthode converge.

\subsubsection{Stabilité de la méthode}
Si f est lipschitzienne, alors la méthode converge.\\
Définissons pour cela deux méthodes :
\[\left\{ \begin{array}{c c c}
	y_{j+1} &=& y_j + \frac{h}{2}[3f(x_j,y_j)-f(x_{j-1},y_{j-1})]\\
	y(x_0) &=& y_0
\end{array}\right.\]
\[\left\{ \begin{array}{c c c}
	z_{j+1} &=& z_j + \frac{h}{2}[3f(x_j,z_j)-f(x_{j-1},z_{j-1})]+ \varepsilon_j\\
	z(x_0) &=& z_0
\end{array}\right.\]

\begin{eqnarray*}
	|y_{j+1}-z_{j+1}| &\leq& |y_j-z_j| + \frac{h}{2}[3|f(x_j,y_j)-f(x_j,z_j)| + |f(x_{j-1},y_{j-1})-f(x_{j-1},z_{j-1})|]+|\varepsilon_j| \\
	&\leq& |y_j-z_j| + \frac{b-a}{2}[3M|y_j-z_j| + M|y_{j-1}-z_{j-1}|]+|\varepsilon_j| \text{ car } f \text{ lipschitzienne} \\
	&\leq& C(|y_j-z_j|+ |y_{j-1}-z_{j-1}| + |\varepsilon_j|) \text{ avec } C = \frac{3}{2}(b-a)M+1 > 0 \\
	&\leq& C((1+C)|y_{j-1}-z_{j-1}| + |y_{j-2}-z_{j-2}| + |\varepsilon_j| + |\varepsilon_{j-1}|) \\
	&\leq& C'(|y_{j-1}-z_{j-1}| + |y_{j-2}-z_{j-2}| + |\varepsilon_j| + |\varepsilon_{j-1}|) \\
	&\leq& \vdots \\
	&\leq& C''\left(|y_1-z_1| + |y_0-z_0| + \sum_{i=1}^j |\varepsilon_i|\right) \\
\end{eqnarray*}

Or, on a $|y_1-z-1|\leq C(|y_0-z_0| + |\varepsilon_0|)$ car la méthode est stable. Ainsi :
\[|y_{j+1}-z_{j+1}|\leq C^{(3)}\left(|y_0-z_0| + \sum_{i=0}^j |\varepsilon_i|\right)\]

D'où :
\[\max_{0\leq j\leq n} |y_j - z_j| \leq C^{(3)}\left(|y_0-z_0| + \sum_{i=0}^{n-1} |\varepsilon_i|\right)\]

\bigskip
On peut également utiliser le théorème suivant : \\
Une méthode est stable si les racines du polynôme $\rho(\lambda)=0$ sont à l'intérieur du cercle unité ou elles sont simples si elles sont sur le cercle unité du plan complexe.\\
Le polynôme $\rho(\lambda)$ est défini par : \[\rho(\lambda)=\sum_{i=1}^k \alpha_i \lambda^i\]
où les $\alpha_i$ sont les coefficients devant les $y_{n-i}$

\bigskip
Le polynôme est dans notre cas : \[\lambda^2-\lambda=\lambda(\lambda-1)\]
Les racines sont donc 0 et 1, qui sont simples. La méthode est donc bien stable.

\subsubsection{Consistance de la méthode}
On utilise pour cela le théorème suivant :
Si on vérifie les conditions suivantes :
\[\sum_{i=1}^k \alpha_i = 0 \text{ et } \sum_{i=1}^k i\alpha_i + \beta_i =0\]
où $\alpha_i$ est définie comme précédemment et les $\beta_i$ sont les coefficients devant les $\beta_i$, alors la méthode est consistante. 

\bigskip
Dans notre exemple, on a la méthode : 
\[y_{t+1}=y_t + \frac{h}{2}(3f_t - f_{t-1})\]
D'où :
\[\alpha_1 = 1,\ \alpha_2=-1\]
et :
\[\beta_1 = \frac{3}{2},\ \beta_2=-\frac{1}{2}\]

On a donc : $\sum \alpha_i = 0$ et :
\[\sum i\alpha_i + \beta_i = 1-2+\frac{3}{2}-\frac{1}{2} = 0\]

La méthode est donc consistante. \\
Comme on l'a retrouvé précédemment, la méthode est bien convergente, car elle est consistante et stable.
