\part{Les équations linéaires}

\Def{Équations linéaires homogènes}{$\dot{x}(t)=Ax(t)$ où 
$x=\begin{pmatrix} 
x_1 \\
\vdots \\
x_n 
\end{pmatrix}\in\mathbb{R}^n,\ A\in\mathcal{M}_n(\mathbb{R})$
Pour $1\leq i \leq n$ : \[\dot{x_i} = \sum_{j=1}^n a_{ij} x_j \text{ où } A=(a_{ij})_{1\leq i,j\leq n}\]}

Les équations sont :
\begin{itemize}
	\item linéaires
	\item homogènes
	\item autonomes
\end{itemize}

\bigskip
Le problème de Cauchy possède-t-il une solution ?\\
D'après le théorème de Picard-Linderlöf, $\dot{x}=Ax$, $x(0)=x_0$ possède une solution, car : 
\begin{itemize}
	\item $f(x)=Ax\in\mathcal{C}^0(\mathbb{R}^n)$
	\item $f(x)$ est lipschitzienne car : \[||f(x)-f(\tilde{x})||=||A(x-\tilde{x})||\leq ||A||\times||x-\tilde{x}||\]
\end{itemize}

Alors $x(t,x_0)$ existe, est unique et définie globalement. Mais comment trouver $x(t,x_0)$ ?

\Def{Exponentielle de matrice}{\[e^A=\sum_{i=0}^{\infty} \frac{A^i}{i!}\]}

\Prop{}{$e^A$ est bien définie, ie $e^A:\mathbb{R}^n\to\mathbb{R}^n$ est une application linéaire bien définie}

\begin{dem}
Pour montrer la convergence, regardons la somme partielle : 
\begin{eqnarray*}
	||x+Ax+...+\frac{A^k}{k!}x||&\leq&||Id+A+...+\frac{A^k}{k!}||\times||x|| \\
				     &\leq&\sum_{i=0}^k ||\frac{A^k}{k!}||\times ||x|| \\
		       		     &\leq& \sum_{i=0}^l \frac{a^k}{k!} \times ||x|| \text{ en posant } a=||A||\\
				     &\leq& e^a ||x||
\end{eqnarray*}
D'où : \[\frac{||e^Ax||}{||x||}\leq e^a\]
Donc $e^A$ est bien définie. La linéarité est immédiate. QED.
\end{dem}

\Theo{}{La solution $x(t,x_0)$ de $\dot{x}=Ax$ est $x(t,x_0)=e^{At}x_0$}

\begin{dem}
\begin{eqnarray*}
	\frac{d}{dt} x(t,x_0) &=& \frac{d}{dt} \left(Id + At + \frac{A^2t^2}{2!}+\frac{A^3t^3}{3!}+...\right) x_0\\
			      &=& \left(A+A^2t+\frac{A^3t^2}{2!} + ...\right) x_0 \\
			      &=& Ae^{At}x_0 \\
			      &=& Ax(t,x_0)
\end{eqnarray*}
\end{dem}

\section{Changement de repère}
On pose $y=Tx$ où $T:\mathbb{R}^n\to\mathbb{R}^n$ inversible, $T\in\mathcal{M}_n(\mathbb{R})$\\
Ainsi, $\dot{y}=TAT^{-1}y=\tilde{A}y$

De même; on pose $w=Tz$, ce qui nous donne : $\dot{w}=TAT^{-1}w$

\Theo{}{\[e^{\tilde{A}t}=Te^{At}T^{-1}\]
La solution transformée résoud l'équation transformée.}

\begin{dem}
\begin{eqnarray*}
	e^{\tilde{A}t}&=&e^{TAT^{-1}t}\\
		&=&Id+TAT^{-1}t+\frac{(TAT^{-1}t)^2}{2!}+\frac{(TAT^{-1}t)^3}{3!}+...\\
		&=&Id+TAT^{-1}t+\frac{TA^2T^{-1}t^2}{2!}+\frac{TA^3T^{-1}t^3}{3!}+...
\end{eqnarray*}
\begin{eqnarray*}
	Te^{At}T^{-1}&=&T\left(Id+At+\frac{A^2t^2}{2!}+\frac{A^3t^3}{3!}+...\right)T^{-1}\\
		&=&Id+TAT^{-1}t+\frac{TA^2Tt^2}{2!}+\frac{TA^3T^{-1}t^3}{3!}+...
\end{eqnarray*}
\end{dem}

\section{Comment calculer $e^{At}$ ?\\}
Supposons A diagonalisable. $\exists T:\mathbb{R}^n\to\mathbb{R}^n$ :
\[\tilde{A}=TAT^{-1}=\begin{pmatrix}\lambda_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \lambda_n \end{pmatrix}\]
$\Rightarrow T^{-1}e^{\tilde{A}t}T=e^{At}$, avec :
\[e^{\tilde{A}t}=\begin{pmatrix}e^{\lambda_1 t} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & e^{\lambda_n t} \end{pmatrix}\]

Maintenant, si A n'est pas diagonalisable :
\Theo{de Jordan}{Il existe $T:\mathbb{C}^n\to\mathbb{C}^n$ tel que : 
\[TAT^{-1}=\tilde{A}=
\begin{pmatrix}
	D_1 & 0 & \cdots & \cdots & \cdots & 0 \\
	\vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
	0 & \cdots & D_L & 0 & \cdots & 0 \\
	0 & \cdots & 0 & J_1 & \cdots & 0 \\
	\vdots & \vdots & \vdots &\vdots & \ddots & \vdots \\
	0 & \cdots & \cdots & \cdots & 0 & J_K
\end{pmatrix}	
\]où :
\[D_j=\underbrace{\begin{pmatrix} \lambda_j & & 0 \\ & \ddots & \\ 0 &  & \lambda_j \end{pmatrix}}_{v_j}
J_k=\underbrace{\begin{pmatrix}\lambda_k & 1 &  & 0 \\  & \ddots & \ddots &  \\  &  & \ddots & 1 \\ 0& & & \lambda_k \end{pmatrix}}_{\mu_j}\]
avec $D_j$ et $J_k$ deux matrices carrées dont la taille est celle de la multiplicité de la racine ($v_j$ et $\mu_j$).}

$n=v_1+...+v_L+\mu_1+...+\mu_k$\\
A chaque bloc $D_j$ correspond $v_j$ vecteurs propres. (vecteurs de la base canonique)\\
A chaque $J_K$ correspond un vecteur propre.\\
A chaque valeur propre $\lambda_k$ de multiplicité $\mu_k$ correspond un bloc de Jordan s'il y a moins de $\mu_k$ vecteurs propres indépendants correspondant à $\lambda_k$.

\Exemp{}{Soit n=1, $\lambda$ une valeur propre de multiplicité 4.
\begin{itemize}
	\item 1er cas : 4 vecteurs propres indépendants :
		\[\tilde{A}=\begin{pmatrix} \lambda & & & 0 \\ & \lambda & & \\ & & \lambda & \\ 0 & & & \lambda \end{pmatrix}\]
	\item 2ème cas : 3 vecteurs propres indépendants $e_1,e_2,e_3$ :
		\[\begin{pmatrix} \lambda & & & \\ & \lambda & & \\ & & \lambda & 1 \\ & & & \lambda\end{pmatrix}\]
	\item 3ème cas : 2 vecteurs indépendants $e_1$ et $e_3$ :
		\[\begin{pmatrix} \lambda & 1 & & \\ & \lambda & & \\ & & \lambda & 1 \\ & & & \lambda\end{pmatrix}\]
	\item 4ème cas : 2 vecteurs indépendants $e_1$ et $e_3$ :
		\[\begin{pmatrix} \lambda & & & \\ & \lambda & 1 & \\ & & \lambda & 1 \\ & & & \lambda\end{pmatrix}\]
	\item 5ème cas : 1 seul vecteur indépendant $e_1$ :
		\[\begin{pmatrix} \lambda & 1 & & \\ & \lambda & 1 & \\ & & \lambda & 1 \\ & & & \lambda\end{pmatrix}\]
\end{itemize}
}

\Lem{}{\[e^{\tilde{A}}=\begin{pmatrix}e^{D_1} & & & & & 0 \\ & \ddots & & & & \\ & & e^{D_L} & & & \\ & & & e^{J_1} & & \\ & & & & \ddots & \\ 0& & & & & e^{J_k}\end{pmatrix}\]}
\begin{dem}
Soit $\tilde{A}=\begin{pmatrix} A_1 & 0 \\ 0 & A_2 \end{pmatrix}$.
\[\tilde{A}^2=\begin{pmatrix}A_1&0\\0&A_2 \end{pmatrix}\begin{pmatrix}A_1&0\\0&A_2\end{pmatrix}=\begin{pmatrix}A_1^2&0\\0&A_2^2\end{pmatrix}\]
\[\tilde{A}^k=\begin{pmatrix}A_1^k&0\\0&A_2^k\end{pmatrix}\]
	Tous les résultats s'en suivent par récurrence immédiate.
\end{dem}

\bigskip
\Prop{}{Soit $J=\lambda Id + N$ où \[N=\begin{pmatrix}0 & 1 & & 0\\ & \ddots & \ddots & \\ & & \ddots & 1 \\ 0 & & & 0\end{pmatrix}\]
	alors : 
\[e^{Jt}=\begin{pmatrix}e^{\lambda t} & te^{\lambda t} & \cdots & \frac{t^{n-1}}{(n-1)!}e^{\lambda t} \\ & \ddots & \ddots & \vdots \\ & & \ddots & te^{\lambda t} \\ 0 & & & e^{\lambda t}\end{pmatrix}\]}
\begin{dem}
	Si AB=BA $\Rightarrow\ e^{A+B}=e^Ae^B$
	N est une matrice nilpotente : on a $N^{\mu}=0$, avec $\mu$ la taille de la matrice.
	\[e^{Nt}=\begin{pmatrix}1 &    t   & \frac{t^2}{2!}  & \cdots & \frac{t^{\mu-1}}{(\mu-1)!} \\
				  & \ddots & \ddots & \ddots & \vdots           \\ 
				  &        & \ddots & \ddots & \frac{t^2}{2!}        \\
      				  &        &        & \ddots &       t               \\
				0 &        &        &        &       1               
	\end{pmatrix}\]
\end{dem}

\section{Résolution des systèmes}

\bigskip
$\dot{y}=\tilde{A}y$ admet donc comme solution au problème de Cauchy $y(t)=e^{\tilde{A}t}y_0$\\
$y(t)\in\mathbb{C}^n$ est une combinaison linéaire de $e^{\lambda_j t}$, $1\leq j\leq L$ et de $e^{\lambda_i t},te^{\lambda_i t},...,t^{\mu_i-1}e^{\lambda_i t}$, $1\leq i\leq k$
\Lem{}{Si $y(t)\in\mathbb{C}^n$ est une solution de $\dot{y}=Ay$ alors $\Re(y(t))\in\mathbb{R}^n$ et $\Im(y(t))\in\mathbb{R}^n$ sont des solutions de $\dot{y}=Ay$, $y\in\mathbb{R}^n$}

\begin{dem}
	On a $y(t)=v(t)=iw(t)\in\mathbb{C}^n$ et :
	\begin{eqnarray*}
		\dot{y}(t)&=&\frac{d}{dt}(v(t)+iw(t))\\
			&=&\frac{d}{dt}v(t) + i\frac{d}{dt}w(t)\\
			&=&A(v(t)+iw(t))\\
			&=&Av(t) + iAw(t)
	\end{eqnarray*}
	D'où $\dot{v}(t)=Av(t)$ et $\dot{w}(t)=Aw(t)$
\end{dem}

Conclusion : $y(t)\in\mathbb{R}^n$ solution de $\dot{y}(t)=Ay(t)$, $y\in\mathbb{R}^n$ est exprimé par :
\begin{itemize}
	\item Si $\lambda_j\in\mathbb{R}$ : $e^{\lambda_j t}$
	\item Si $\lambda_j=\alpha_j+i\beta_j\in\mathbb{C}$ : $e^{(\alpha_j+i\beta_j)t}=e^{\alpha_j t}(\cos \beta_j t + i\sin \beta_j t)$
\end{itemize}

\bigskip
Ainsi, par la conclusion précédente sur les combinaisons linéaires :
\begin{itemize} 
	\item $\lambda_j\in\mathbb{R}$ : $e^{\lambda_j t},\ te^{\lambda_j t},...,\ t^{\mu_j-1}e^{\lambda_j-1}$
	\item $\lambda_j\in\mathbb{C}$ : $e^{\alpha_j t}\cos \beta_k t,...,t^{\mu_k-1}\cos(\beta_kt) e^{\alpha_k t}$ et $e^{\alpha_j t}\sin \beta_k t,...,t^{\mu_k-1}\sin(\beta_kt) e^{\alpha_k t}$
\end{itemize}
$x=Ty$ est combinaire linéaire de tout cela.

\Def{Trace d'une matrice}{\[\text{Tr}(A)=\sum_{i=1}^n a_{ii}\]}

\Prop{}{Tr(A) ne dépend pas de la base choisie.}
\begin{dem}
	\begin{eqnarray*}
	\det(A-\lambda Id)&=&\begin{vmatrix} a_{11}-\lambda & &a_{ij} \\ & \ddots & \\ a_{ij} & & a_{nn}-\lambda \end{vmatrix}\\
			&=&(-\lambda)^n + (-\lambda)^{n-1}\sum_{i=1}^n a_{ii} + ... \\
			&=&(\lambda_1-\lambda)(\lambda_2-\lambda)...(\lambda_n-\lambda)\\
			&=&(-\lambda)^n+(-\lambda)^{n-1}\sum_{i=1}^n \lambda_i + ...
	\end{eqnarray*}
	\[\Rightarrow \sum_{i=1}^n a_{ii}=\sum_{i=1}^n \lambda_i\]
\end{dem}

\Coro{}{\begin{enumerate}
		\item Tr(A)=$\sum_{i=1}^n \lambda_i$
		\item det(A)=$\prod_{i=1}^n \lambda_i$
\end{enumerate}}

\Prop{}{\begin{enumerate}
		\item $e^{\text{Tr}(A)}=\det e^A$
		\item $e^A$ est toujours inversible
		\item $e^A$ préserve l'orientation
		\item $A=-A^T$ (A antisymétrique) $\Rightarrow\ \det e^A=1$
		\item $A=-A^T\ \Rightarrow\ e^A$ est une matrice orthogonale
\end{enumerate}}
\begin{dem}
1. $\exists T : \mathbb{C}^n\to \mathbb{C}^n$; \[TAT^{-1}=\tilde{A}=\begin{pmatrix}\lambda_1 & * & \cdots & * \\ & \ddots & & \vdots \\ & & \ddots & \vdots \\ 0 & & & \lambda_n \end{pmatrix}\]
	\[\text{Tr}(\tilde{A})=\sum_{i=1}^n \lambda_i \Rightarrow e^{\text{Tr}(\tilde{A})}=\prod_{i=1}^n e^{\lambda_i}\]
\[e^{\tilde{A}}=\begin{pmatrix}e^{\lambda_1} & * & \cdots & * \\ & \ddots & & \vdots \\ & & \ddots & \vdots \\ 0 & & & e^{\lambda_n} \end{pmatrix}\]
	\[\det e^{\tilde{A}}=\prod_{i=1}^n e^{\lambda_i} = \text{Tr}(\tilde{A})\]

On a $A=T^{-1}AT$
\[\det(e^A)=\det(e^{T^{-1}\tilde{A}T})=\det(T^{-1}e^{\tilde{A}}T)=\det e^{\tilde{A}}=e^{\sum_{i=1}^n \lambda_i} = e^{\text{Tr}(A)}\]

\bigskip
2. $\det e^A >0$ (d'après 1) $\Rightarrow\ e^A$ inversible.

\bigskip
3. Soit $\mathbb{R}^n$ accompagné de sa base $(B_1,...,B_n)=\mathcal{B}$.\\
$\{\mathcal{B}\}$ : ensemble des bases.
\[\{\mathcal{B}\}=\{\mathcal{B}'\}\cup\{\mathcal{B}''\}\]
$\mathcal{B}'$ : base directe. $\mathcal{B}''$ : base indirecte.

\[B_1' \xrightarrow{T} B_2'\ :\ \det T>0\]
\[B_1'' \xrightarrow{T} B_2''\ :\ \det T>0\]
\[B_1' \xrightarrow{T} B_1''\ :\ \det T<0\]

$\det e^A >0\ \Rightarrow e^A:\mathbb{R}^n\to\mathbb{R}^n$ conserve l'orientation.

4. On a forcément \[A=\begin{pmatrix} 0 & & a_{ij} \\ & \ddots & \\ -a_{ji} & & 0 \end{pmatrix}\]
\[\text{Tr}(A)=\sum_{i=1}^n 0 = 0 \Rightarrow \det e^A = e^{\text{Tr}(A)}=1\]

5. \begin{eqnarray*}
	A+A^T=0 \Rightarrow e^{A+A^T}&=&Id \\ &=&e^Ae^{A^T}
\end{eqnarray*}
Or, $\left(e^A\right)^T=e^{A^T}$, donc :
\[Id=e^A\left(e^A\right)^T \Rightarrow e^A \text{ orthogonale}\]
\end{dem}

\bigskip
\Theo{de Liouville}{On passe d'un espace $B$ à $\tilde{B}$ défini ainsi :
	\[\tilde{B}=\{\gamma_t(x_0),x_0\in B\}=\{e^{At}x_0,x_0\in B\} = e^{At}B\]
	On a alors :
\[\text{Vol}(\tilde{B})=e^{\text{Tr}(A)t}\text{Vol}(B)\]
}

\begin{dem}
	On prend $B=B_{\rho}=(\rho_1,...,\rho_n)$
	\begin{eqnarray*}
		\text{Vol}(\tilde{B}_{\rho})&=&\text{Vol}(e^{At}B_{\rho})\\
					&=&\det(e^{At}\rho)\\
					&=&\det(e^At)\times\det(\rho)\\
					&=&e^{\text{tr}(A)t}\times\det(\rho)\\
					&=&e^{\text{Tr}(A)t}\times \text{Vol}(B_{\rho})
	\end{eqnarray*}
\end{dem}
